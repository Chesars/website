{
  "name": "outsource",
  "slug": "gwbischof__outsource-mcp",
  "description": "Give your AI assistant its own AI assistants.",
  "readme": "# Outsource MCP\n\nAn MCP (Model Context Protocol) server that enables AI applications to outsource tasks to various model providers through a unified interface.\n\n<img width=\"1154\" alt=\"image\" src=\"https://github.com/user-attachments/assets/cd364a7c-eae5-4c58-bc1f-fdeea6cb8434\" />\n\n<img width=\"1103\" alt=\"image\" src=\"https://github.com/user-attachments/assets/55924981-83e9-4811-9f51-b049595b7505\" />\n\n\nCompatible with any AI tool that supports the Model Context Protocol, including Claude Desktop, Cline, and other MCP-enabled applications.\nBuilt with [FastMCP](https://github.com/mcp/fastmcp) for the MCP server implementation and [Agno](https://github.com/agno-agi/agno) for AI agent capabilities.\n\n## Features\n\n- 🤖 **Multi-Provider Support**: Access 20+ AI providers through a single interface\n- 📝 **Text Generation**: Generate text using models from OpenAI, Anthropic, Google, and more\n- 🎨 **Image Generation**: Create images using DALL-E 3 and DALL-E 2\n- 🔧 **Simple API**: Consistent interface with just three parameters: provider, model, and prompt\n- 🔑 **Flexible Authentication**: Only configure API keys for the providers you use\n\n## Configuration\n\nAdd the following configuration to your MCP client. Consult your MCP client's documentation for specific configuration details.\n\n```json\n{\n  \"mcpServers\": {\n    \"outsource-mcp\": {\n      \"command\": \"uvx\",\n      \"args\": [\"--from\", \"git+https://github.com/gwbischof/outsource-mcp.git\", \"outsource-mcp\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-openai-key\",\n        \"ANTHROPIC_API_KEY\": \"your-anthropic-key\",\n        \"GOOGLE_API_KEY\": \"your-google-key\",\n        \"GROQ_API_KEY\": \"your-groq-key\",\n        \"DEEPSEEK_API_KEY\": \"your-deepseek-key\",\n        \"XAI_API_KEY\": \"your-xai-key\",\n        \"PERPLEXITY_API_KEY\": \"your-perplexity-key\",\n        \"COHERE_API_KEY\": \"your-cohere-key\",\n        \"FIREWORKS_API_KEY\": \"your-fireworks-key\",\n        \"HUGGINGFACE_API_KEY\": \"your-huggingface-key\",\n        \"MISTRAL_API_KEY\": \"your-mistral-key\",\n        \"NVIDIA_API_KEY\": \"your-nvidia-key\",\n        \"OLLAMA_HOST\": \"http://localhost:11434\",\n        \"OPENROUTER_API_KEY\": \"your-openrouter-key\",\n        \"TOGETHER_API_KEY\": \"your-together-key\",\n        \"CEREBRAS_API_KEY\": \"your-cerebras-key\",\n        \"DEEPINFRA_API_KEY\": \"your-deepinfra-key\",\n        \"SAMBANOVA_API_KEY\": \"your-sambanova-key\"\n      }\n    }\n  }\n}\n```\n\nNote: The environment variables are optional. Only include the API keys for the providers you want to use.\n\n## Quick Start\n\nOnce installed and configured, you can use the tools in your MCP client:\n\n1. **Generate text**: Use the `outsource_text` tool with provider \"openai\", model \"gpt-4o-mini\", and prompt \"Write a haiku about coding\"\n2. **Generate images**: Use the `outsource_image` tool with provider \"openai\", model \"dall-e-3\", and prompt \"A futuristic city skyline at sunset\"\n\n## Tools\n\n### outsource_text\nCreates an Agno agent with a specified provider and model to generate text responses. \n\n**Arguments:**\n- `provider`: The provider name (e.g., \"openai\", \"anthropic\", \"google\", \"groq\", etc.)\n- `model`: The model name (e.g., \"gpt-4o\", \"claude-3-5-sonnet-20241022\", \"gemini-2.0-flash-exp\")\n- `prompt`: The text prompt to send to the model\n\n### outsource_image\nGenerates images using AI models.\n\n**Arguments:**\n- `provider`: The provider name (currently only \"openai\" is supported)\n- `model`: The model name (\"dall-e-3\" or \"dall-e-2\")\n- `prompt`: The image generation prompt\n\nReturns the URL of the generated image.\n\n> **Note**: Image generation is currently only supported by OpenAI models (DALL-E 2 and DALL-E 3). Other providers only support text generation.\n\n## Supported Providers\n\nThe following providers are supported. Use the provider name (in parentheses) as the `provider` argument:\n\n### Core Providers\n- **OpenAI** (`openai`) - GPT-4, GPT-3.5, DALL-E, etc. | [Models](https://platform.openai.com/docs/models)\n- **Anthropic** (`anthropic`) - Claude 3.5, Claude 3, etc. | [Models](https://docs.anthropic.com/en/docs/about-claude/models/overview)\n- **Google** (`google`) - Gemini Pro, Gemini Flash, etc. | [Models](https://ai.google.dev/models)\n- **Groq** (`groq`) - Llama 3, Mixtral, etc. | [Models](https://console.groq.com/docs/models)\n- **DeepSeek** (`deepseek`) - DeepSeek Chat & Coder | [Models](https://api-docs.deepseek.com/api/list-models)\n- **xAI** (`xai`) - Grok models | [Models](https://docs.x.ai/docs/models)\n- **Perplexity** (`perplexity`) - Sonar models | [Models](https://docs.perplexity.ai/guides/model-cards)\n\n### Additional Providers\n- **Cohere** (`cohere`) - Command models | [Models](https://docs.cohere.com/v2/docs/models)\n- **Mistral AI** (`mistral`) - Mistral Large, Medium, Small | [Models](https://docs.mistral.ai/getting-started/models/models_overview/)\n- **NVIDIA** (`nvidia`) - Various models | [Models](https://build.nvidia.com/models)\n- **HuggingFace** (`huggingface`) - Open source models | [Models](https://huggingface.co/models)\n- **Ollama** (`ollama`) - Local models | [Models](https://ollama.com/library)\n- **Fireworks AI** (`fireworks`) - Fast inference | [Models](https://fireworks.ai/models?view=list)\n- **OpenRouter** (`openrouter`) - Multi-provider access | [Models](https://openrouter.ai/docs/overview/models)\n- **Together AI** (`together`) - Open source models | [Models](https://docs.together.ai/docs/serverless-models)\n- **Cerebras** (`cerebras`) - Fast inference | [Models](https://cerebras.ai/models)\n- **DeepInfra** (`deepinfra`) - Optimized models | [Models](https://deepinfra.com/docs/models)\n- **SambaNova** (`sambanova`) - Enterprise models | [Models](https://docs.sambanova.ai/cloud/docs/get-started/supported-models)\n\n### Enterprise Providers\n- **AWS Bedrock** (`aws` or `bedrock`) - AWS-hosted models | [Models](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html)\n- **Azure AI** (`azure`) - Azure-hosted models | [Models](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/foundry-models-overview)\n- **IBM WatsonX** (`ibm` or `watsonx`) - IBM models | [Models](https://www.ibm.com/docs/en/software-hub/5.1.x?topic=install-foundation-models)\n- **LiteLLM** (`litellm`) - Universal interface | [Models](https://docs.litellm.ai/docs/providers)\n- **Vercel v0** (`vercel` or `v0`) - Vercel AI | [Models](https://sdk.vercel.ai/docs/introduction)\n- **Meta Llama** (`meta`) - Direct Meta access | [Models](https://www.llama.com/get-started/)\n\n### Environment Variables\n\nEach provider requires its corresponding API key:\n\n| Provider | Environment Variable | Example |\n|----------|---------------------|---------|\n| OpenAI | `OPENAI_API_KEY` | sk-... |\n| Anthropic | `ANTHROPIC_API_KEY` | sk-ant-... |\n| Google | `GOOGLE_API_KEY` | AIza... |\n| Groq | `GROQ_API_KEY` | gsk_... |\n| DeepSeek | `DEEPSEEK_API_KEY` | sk-... |\n| xAI | `XAI_API_KEY` | xai-... |\n| Perplexity | `PERPLEXITY_API_KEY` | pplx-... |\n| Cohere | `COHERE_API_KEY` | ... |\n| Fireworks | `FIREWORKS_API_KEY` | ... |\n| HuggingFace | `HUGGINGFACE_API_KEY` | hf_... |\n| Mistral | `MISTRAL_API_KEY` | ... |\n| NVIDIA | `NVIDIA_API_KEY` | nvapi-... |\n| Ollama | `OLLAMA_HOST` | http://localhost:11434 |\n| OpenRouter | `OPENROUTER_API_KEY` | ... |\n| Together | `TOGETHER_API_KEY` | ... |\n| Cerebras | `CEREBRAS_API_KEY` | ... |\n| DeepInfra | `DEEPINFRA_API_KEY` | ... |\n| SambaNova | `SAMBANOVA_API_KEY` | ... |\n| AWS Bedrock | AWS credentials | Via AWS CLI/SDK |\n| Azure AI | Azure credentials | Via Azure CLI/SDK |\n| IBM WatsonX | `IBM_WATSONX_API_KEY` | ... |\n| Meta Llama | `LLAMA_API_KEY` | ... |\n\n**Note**: Only configure the API keys for providers you plan to use.\n\n## Examples\n\n### Text Generation\n```\n# Using OpenAI\nprovider: openai\nmodel: gpt-4o-mini\nprompt: Write a haiku about coding\n\n# Using Anthropic\nprovider: anthropic\nmodel: claude-3-5-sonnet-20241022\nprompt: Explain quantum computing in simple terms\n\n# Using Google\nprovider: google\nmodel: gemini-2.0-flash-exp\nprompt: Create a recipe for chocolate chip cookies\n```\n\n### Image Generation\n```\n# Using DALL-E 3\nprovider: openai\nmodel: dall-e-3\nprompt: A serene Japanese garden with cherry blossoms\n\n# Using DALL-E 2\nprovider: openai\nmodel: dall-e-2\nprompt: A futuristic cityscape at sunset\n```\n\n## Development\n\n### Prerequisites\n\n- Python 3.11 or higher\n- [uv](https://github.com/astral-sh/uv) package manager\n\n### Setup\n\n```bash\ngit clone https://github.com/gwbischof/outsource-mcp.git\ncd outsource-mcp\nuv sync\n```\n\n### Testing with MCP Inspector\n\nThe MCP Inspector allows you to test the server interactively:\n\n```bash\nmcp dev server.py\n```\n\n### Running Tests\n\nThe test suite includes integration tests that verify both text and image generation:\n\n```bash\n# Run all tests\nuv run pytest\n```\n\n**Note:** Integration tests require API keys to be set in your environment.\n\n## Troubleshooting\n\n### Common Issues\n\n1. **\"Error: Unknown provider\"**\n   - Check that you're using a supported provider name from the list above\n   - Provider names are case-insensitive\n\n2. **\"Error: OpenAI API error\"** \n   - Verify your API key is correctly set in the environment variables\n   - Check that your API key has access to the requested model\n   - Ensure you have sufficient credits/quota\n\n3. **\"Error: No image was generated\"**\n   - This can happen if the image generation request fails\n   - Try a simpler prompt or different model (dall-e-2 vs dall-e-3)\n\n4. **Environment variables not working**\n   - Make sure to restart your MCP client after updating the configuration\n   - Verify the configuration file location for your specific MCP client\n   - Check that the environment variables are properly formatted in the configuration\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n",
  "category": null,
  "qualityScore": 68,
  "githubUrl": "https://github.com/gwbischof/outsource-mcp",
  "programmingLanguage": "Python",
  "gitHubOrg": "gwbischof",
  "gitHubRepo": "outsource-mcp",
  "repositoryPath": null,
  "gh_stars": 10,
  "gh_contributors": 1,
  "gh_issues": 0,
  "gh_releases": false,
  "gh_ci_cd": false,
  "gh_latest_commit_hash": "70e7b6b4031d1819fad18ee313fb6ec16e126b92",
  "last_scraped_at": "2025-08-01T13:16:29.732Z",
  "implementing_tools": null,
  "implementing_prompts": null,
  "implementing_resources": null,
  "implementing_sampling": null,
  "implementing_roots": null,
  "implementing_logging": null,
  "implementing_stdio": null,
  "implementing_streamable_http": null,
  "implementing_oauth2": null
}